{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8bcff235",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-06-26T22:11:16.315409Z",
     "iopub.status.busy": "2024-06-26T22:11:16.315022Z",
     "iopub.status.idle": "2024-06-26T22:11:18.061594Z",
     "shell.execute_reply": "2024-06-26T22:11:18.060497Z"
    },
    "papermill": {
     "duration": 1.752339,
     "end_time": "2024-06-26T22:11:18.063758",
     "exception": false,
     "start_time": "2024-06-26T22:11:16.311419",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'ladi-vton-hunting'...\r\n",
      "remote: Enumerating objects: 186, done.\u001b[K\r\n",
      "remote: Counting objects: 100% (85/85), done.\u001b[K\r\n",
      "remote: Compressing objects: 100% (45/45), done.\u001b[K\r\n",
      "remote: Total 186 (delta 75), reused 40 (delta 40), pack-reused 101\u001b[K\r\n",
      "Receiving objects: 100% (186/186), 1.56 MiB | 16.11 MiB/s, done.\r\n",
      "Resolving deltas: 100% (93/93), done.\r\n"
     ]
    }
   ],
   "source": [
    "import git\n",
    "!git clone https://github.com/SEmohamedAhmed/ladi-vton-hunting.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe1b534d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-26T22:11:18.070632Z",
     "iopub.status.busy": "2024-06-26T22:11:18.070304Z",
     "iopub.status.idle": "2024-06-26T22:14:20.076985Z",
     "shell.execute_reply": "2024-06-26T22:14:20.075959Z"
    },
    "papermill": {
     "duration": 182.012806,
     "end_time": "2024-06-26T22:14:20.079375",
     "exception": false,
     "start_time": "2024-06-26T22:11:18.066569",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting diffusers==0.14.0\r\n",
      "  Downloading diffusers-0.14.0-py3-none-any.whl.metadata (32 kB)\r\n",
      "Collecting transformers==4.27.3\r\n",
      "  Downloading transformers-4.27.3-py3-none-any.whl.metadata (106 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.7/106.7 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting accelerate==0.18.0\r\n",
      "  Downloading accelerate-0.18.0-py3-none-any.whl.metadata (16 kB)\r\n",
      "Collecting clean-fid==0.1.35\r\n",
      "  Downloading clean_fid-0.1.35-py3-none-any.whl.metadata (36 kB)\r\n",
      "Collecting torchmetrics==0.11.4 (from torchmetrics[image]==0.11.4)\r\n",
      "  Downloading torchmetrics-0.11.4-py3-none-any.whl.metadata (15 kB)\r\n",
      "Collecting wandb==0.14.0\r\n",
      "  Downloading wandb-0.14.0-py3-none-any.whl.metadata (7.9 kB)\r\n",
      "Collecting matplotlib==3.7.1\r\n",
      "  Downloading matplotlib-3.7.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\r\n",
      "Collecting xformers\r\n",
      "  Downloading xformers-0.0.26.post1-cp310-cp310-manylinux2014_x86_64.whl.metadata (1.0 kB)\r\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.10/site-packages (from diffusers==0.14.0) (6.11.0)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from diffusers==0.14.0) (3.13.1)\r\n",
      "Requirement already satisfied: huggingface-hub>=0.10.0 in /opt/conda/lib/python3.10/site-packages (from diffusers==0.14.0) (0.21.4)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from diffusers==0.14.0) (1.26.4)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from diffusers==0.14.0) (2023.12.25)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from diffusers==0.14.0) (2.31.0)\r\n",
      "Requirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from diffusers==0.14.0) (9.5.0)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.27.3) (21.3)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.27.3) (6.0.1)\r\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.27.3)\r\n",
      "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\r\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.27.3) (4.66.1)\r\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate==0.18.0) (5.9.3)\r\n",
      "Requirement already satisfied: torch>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.18.0) (2.1.2)\r\n",
      "Requirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (from clean-fid==0.1.35) (0.16.2)\r\n",
      "Requirement already satisfied: scipy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from clean-fid==0.1.35) (1.11.4)\r\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.0 in /opt/conda/lib/python3.10/site-packages (from wandb==0.14.0) (8.1.7)\r\n",
      "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb==0.14.0) (3.1.41)\r\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb==0.14.0) (1.42.0)\r\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from wandb==0.14.0) (0.4.0)\r\n",
      "Collecting pathtools (from wandb==0.14.0)\r\n",
      "  Downloading pathtools-0.1.2.tar.gz (11 kB)\r\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25hRequirement already satisfied: setproctitle in /opt/conda/lib/python3.10/site-packages (from wandb==0.14.0) (1.3.3)\r\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from wandb==0.14.0) (69.0.3)\r\n",
      "Requirement already satisfied: appdirs>=1.4.3 in /opt/conda/lib/python3.10/site-packages (from wandb==0.14.0) (1.4.4)\r\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /opt/conda/lib/python3.10/site-packages (from wandb==0.14.0) (3.20.3)\r\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib==3.7.1) (1.2.0)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib==3.7.1) (0.12.1)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib==3.7.1) (4.47.0)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib==3.7.1) (1.4.5)\r\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib==3.7.1) (3.1.1)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib==3.7.1) (2.9.0.post0)\r\n",
      "Collecting lpips<=0.1.4 (from torchmetrics[image]==0.11.4)\r\n",
      "  Downloading lpips-0.1.4-py3-none-any.whl.metadata (10 kB)\r\n",
      "Collecting torch-fidelity<=0.3.0 (from torchmetrics[image]==0.11.4)\r\n",
      "  Downloading torch_fidelity-0.3.0-py3-none-any.whl.metadata (2.0 kB)\r\n",
      "Collecting torch>=1.4.0 (from accelerate==0.18.0)\r\n",
      "  Downloading torch-2.3.0-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\r\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->accelerate==0.18.0) (4.9.0)\r\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->accelerate==0.18.0) (1.12)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->accelerate==0.18.0) (3.2.1)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->accelerate==0.18.0) (3.1.2)\r\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->accelerate==0.18.0) (2024.3.0)\r\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.4.0->accelerate==0.18.0)\r\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.4.0->accelerate==0.18.0)\r\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.4.0->accelerate==0.18.0)\r\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\r\n",
      "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.4.0->accelerate==0.18.0)\r\n",
      "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\r\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.4.0->accelerate==0.18.0)\r\n",
      "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.4.0->accelerate==0.18.0)\r\n",
      "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.4.0->accelerate==0.18.0)\r\n",
      "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.4.0->accelerate==0.18.0)\r\n",
      "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\r\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.4.0->accelerate==0.18.0)\r\n",
      "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\r\n",
      "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.4.0->accelerate==0.18.0)\r\n",
      "  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\r\n",
      "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.4.0->accelerate==0.18.0)\r\n",
      "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\r\n",
      "Collecting triton==2.3.0 (from torch>=1.4.0->accelerate==0.18.0)\r\n",
      "  Downloading triton-2.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\r\n",
      "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.4.0->accelerate==0.18.0)\r\n",
      "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Requirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb==0.14.0) (1.16.0)\r\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb==0.14.0) (4.0.11)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->diffusers==0.14.0) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->diffusers==0.14.0) (3.6)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->diffusers==0.14.0) (1.26.18)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->diffusers==0.14.0) (2024.2.2)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.10/site-packages (from importlib-metadata->diffusers==0.14.0) (3.17.0)\r\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb==0.14.0) (5.0.1)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.4.0->accelerate==0.18.0) (2.1.3)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.4.0->accelerate==0.18.0) (1.3.0)\r\n",
      "Downloading diffusers-0.14.0-py3-none-any.whl (737 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m737.4/737.4 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading transformers-4.27.3-py3-none-any.whl (6.8 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m92.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading accelerate-0.18.0-py3-none-any.whl (215 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m215.3/215.3 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading clean_fid-0.1.35-py3-none-any.whl (26 kB)\r\n",
      "Downloading torchmetrics-0.11.4-py3-none-any.whl (519 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.2/519.2 kB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading wandb-0.14.0-py3-none-any.whl (2.0 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m72.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading matplotlib-3.7.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m57.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading xformers-0.0.26.post1-cp310-cp310-manylinux2014_x86_64.whl (222.7 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m222.7/222.7 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading torch-2.3.0-cp310-cp310-manylinux1_x86_64.whl (779.1 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m779.1/779.1 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m91.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m73.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m44.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m500.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading triton-2.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (168.1 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.1/168.1 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading lpips-0.1.4-py3-none-any.whl (53 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.8/53.8 kB\u001b[0m \u001b[31m295.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m49.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading torch_fidelity-0.3.0-py3-none-any.whl (37 kB)\r\n",
      "Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m81.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hBuilding wheels for collected packages: pathtools\r\n",
      "  Building wheel for pathtools (setup.py) ... \u001b[?25l-\b \b\\\b \b|\b \bdone\r\n",
      "\u001b[?25h  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8793 sha256=7240a8c8865332b56fff32f10c24fe5498d688f20b792d1c55ddeb66aa1767fb\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/e7/f3/22/152153d6eb222ee7a56ff8617d80ee5207207a8c00a7aab794\r\n",
      "Successfully built pathtools\r\n",
      "Installing collected packages: tokenizers, pathtools, triton, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, matplotlib, wandb, transformers, nvidia-cusolver-cu12, diffusers, torch, xformers, torchmetrics, accelerate, torch-fidelity, lpips, clean-fid\r\n",
      "  Attempting uninstall: tokenizers\r\n",
      "    Found existing installation: tokenizers 0.15.2\r\n",
      "    Uninstalling tokenizers-0.15.2:\r\n",
      "      Successfully uninstalled tokenizers-0.15.2\r\n",
      "  Attempting uninstall: matplotlib\r\n",
      "    Found existing installation: matplotlib 3.7.5\r\n",
      "    Uninstalling matplotlib-3.7.5:\r\n",
      "      Successfully uninstalled matplotlib-3.7.5\r\n",
      "  Attempting uninstall: wandb\r\n",
      "    Found existing installation: wandb 0.16.4\r\n",
      "    Uninstalling wandb-0.16.4:\r\n",
      "      Successfully uninstalled wandb-0.16.4\r\n",
      "  Attempting uninstall: transformers\r\n",
      "    Found existing installation: transformers 4.38.2\r\n",
      "    Uninstalling transformers-4.38.2:\r\n",
      "      Successfully uninstalled transformers-4.38.2\r\n",
      "  Attempting uninstall: torch\r\n",
      "    Found existing installation: torch 2.1.2\r\n",
      "    Uninstalling torch-2.1.2:\r\n",
      "      Successfully uninstalled torch-2.1.2\r\n",
      "  Attempting uninstall: torchmetrics\r\n",
      "    Found existing installation: torchmetrics 1.3.2\r\n",
      "    Uninstalling torchmetrics-1.3.2:\r\n",
      "      Successfully uninstalled torchmetrics-1.3.2\r\n",
      "  Attempting uninstall: accelerate\r\n",
      "    Found existing installation: accelerate 0.28.0\r\n",
      "    Uninstalling accelerate-0.28.0:\r\n",
      "      Successfully uninstalled accelerate-0.28.0\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "beatrix-jupyterlab 2023.128.151533 requires jupyterlab~=3.6.0, but you have jupyterlab 4.1.5 which is incompatible.\r\n",
      "fastai 2.7.14 requires torch<2.3,>=1.10, but you have torch 2.3.0 which is incompatible.\r\n",
      "fitter 1.7.0 requires matplotlib<4.0.0,>=3.7.2, but you have matplotlib 3.7.1 which is incompatible.\r\n",
      "kaggle-environments 1.14.3 requires transformers>=4.33.1, but you have transformers 4.27.3 which is incompatible.\r\n",
      "spopt 0.6.0 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\r\n",
      "ydata-profiling 4.6.4 requires numpy<1.26,>=1.16.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed accelerate-0.18.0 clean-fid-0.1.35 diffusers-0.14.0 lpips-0.1.4 matplotlib-3.7.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105 pathtools-0.1.2 tokenizers-0.13.3 torch-2.3.0 torch-fidelity-0.3.0 torchmetrics-0.11.4 transformers-4.27.3 triton-2.3.0 wandb-0.14.0 xformers-0.0.26.post1\r\n"
     ]
    }
   ],
   "source": [
    "!pip install diffusers==0.14.0 transformers==4.27.3 accelerate==0.18.0 clean-fid==0.1.35 torchmetrics[image]==0.11.4 wandb==0.14.0 matplotlib==3.7.1 xformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1dd3694d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-26T22:14:20.247615Z",
     "iopub.status.busy": "2024-06-26T22:14:20.247276Z",
     "iopub.status.idle": "2024-06-26T22:17:12.713169Z",
     "shell.execute_reply": "2024-06-26T22:17:12.711953Z"
    },
    "papermill": {
     "duration": 172.552612,
     "end_time": "2024-06-26T22:17:12.715720",
     "exception": false,
     "start_time": "2024-06-26T22:14:20.163108",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\r\n",
      "  warn(\r\n",
      "2024-06-26 22:14:28.733365: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "2024-06-26 22:14:28.733495: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "2024-06-26 22:14:28.886378: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "scheduler/scheduler_config.json: 100%|█████████| 308/308 [00:00<00:00, 1.83MB/s]\r\n",
      "text_encoder/config.json: 100%|████████████████| 638/638 [00:00<00:00, 4.12MB/s]\r\n",
      "text_encoder/model.safetensors: 100%|███████| 1.36G/1.36G [00:03<00:00, 354MB/s]\r\n",
      "vae/diffusion_pytorch_model.safetensors: 100%|█| 335M/335M [00:00<00:00, 362MB/s\r\n",
      "vae/config.json: 100%|█████████████████████████| 616/616 [00:00<00:00, 2.97MB/s]\r\n",
      "config.json: 100%|█████████████████████████| 4.72k/4.72k [00:00<00:00, 21.9MB/s]\r\n",
      "model.safetensors: 100%|████████████████████| 3.94G/3.94G [00:10<00:00, 361MB/s]\r\n",
      "Some weights of the model checkpoint at laion/CLIP-ViT-H-14-laion2B-s32B-b79K were not used when initializing CLIPVisionModelWithProjection: ['text_model.encoder.layers.3.self_attn.q_proj.bias', 'text_model.final_layer_norm.bias', 'text_model.encoder.layers.9.self_attn.v_proj.weight', 'text_model.encoder.layers.8.self_attn.out_proj.weight', 'text_model.encoder.layers.16.mlp.fc2.weight', 'text_model.encoder.layers.18.layer_norm2.bias', 'text_model.encoder.layers.10.self_attn.q_proj.weight', 'text_model.encoder.layers.14.self_attn.v_proj.bias', 'text_model.encoder.layers.2.self_attn.out_proj.weight', 'text_model.encoder.layers.18.mlp.fc2.bias', 'text_model.encoder.layers.2.self_attn.k_proj.weight', 'text_model.encoder.layers.20.layer_norm1.bias', 'text_model.encoder.layers.5.layer_norm2.weight', 'text_model.encoder.layers.17.mlp.fc1.bias', 'text_model.encoder.layers.18.mlp.fc1.weight', 'text_model.encoder.layers.10.self_attn.v_proj.weight', 'text_model.encoder.layers.2.mlp.fc1.bias', 'text_model.encoder.layers.4.mlp.fc1.weight', 'text_model.encoder.layers.19.self_attn.out_proj.weight', 'text_model.encoder.layers.4.layer_norm2.weight', 'text_model.encoder.layers.9.self_attn.q_proj.weight', 'text_model.encoder.layers.15.layer_norm1.weight', 'text_model.encoder.layers.10.mlp.fc2.bias', 'text_model.encoder.layers.7.layer_norm2.bias', 'text_model.encoder.layers.8.layer_norm1.bias', 'text_model.encoder.layers.16.self_attn.v_proj.weight', 'text_model.encoder.layers.3.layer_norm1.bias', 'text_model.encoder.layers.8.mlp.fc2.bias', 'text_model.encoder.layers.22.layer_norm1.bias', 'text_model.encoder.layers.16.self_attn.out_proj.bias', 'text_model.encoder.layers.11.self_attn.q_proj.weight', 'text_model.encoder.layers.14.self_attn.k_proj.bias', 'text_model.encoder.layers.15.layer_norm2.bias', 'text_model.encoder.layers.19.mlp.fc2.bias', 'text_model.encoder.layers.9.mlp.fc2.bias', 'text_model.encoder.layers.4.mlp.fc2.weight', 'text_model.encoder.layers.2.layer_norm2.bias', 'text_model.encoder.layers.15.mlp.fc2.bias', 'text_model.encoder.layers.6.self_attn.k_proj.bias', 'text_model.encoder.layers.16.self_attn.v_proj.bias', 'text_model.encoder.layers.23.self_attn.v_proj.weight', 'text_model.encoder.layers.19.self_attn.out_proj.bias', 'text_model.encoder.layers.22.self_attn.out_proj.weight', 'text_model.encoder.layers.20.layer_norm1.weight', 'text_model.encoder.layers.3.self_attn.out_proj.weight', 'text_model.encoder.layers.0.self_attn.v_proj.bias', 'text_model.encoder.layers.14.layer_norm2.bias', 'text_model.encoder.layers.23.layer_norm1.weight', 'text_model.encoder.layers.7.self_attn.q_proj.weight', 'text_model.encoder.layers.9.self_attn.out_proj.weight', 'text_model.encoder.layers.18.layer_norm2.weight', 'text_model.encoder.layers.19.self_attn.q_proj.weight', 'text_model.encoder.layers.19.self_attn.v_proj.weight', 'text_model.encoder.layers.3.self_attn.v_proj.bias', 'text_model.encoder.layers.18.self_attn.out_proj.weight', 'text_model.encoder.layers.18.mlp.fc2.weight', 'text_model.encoder.layers.23.layer_norm1.bias', 'text_model.encoder.layers.2.self_attn.q_proj.weight', 'text_model.encoder.layers.22.mlp.fc1.weight', 'text_model.encoder.layers.19.layer_norm1.weight', 'text_model.encoder.layers.11.self_attn.k_proj.weight', 'text_model.encoder.layers.1.layer_norm1.bias', 'text_model.encoder.layers.14.self_attn.q_proj.bias', 'text_model.encoder.layers.4.layer_norm1.weight', 'text_model.encoder.layers.0.mlp.fc2.weight', 'text_model.encoder.layers.21.self_attn.v_proj.bias', 'text_model.encoder.layers.4.self_attn.v_proj.bias', 'text_model.encoder.layers.8.layer_norm2.weight', 'text_model.encoder.layers.23.self_attn.v_proj.bias', 'text_model.encoder.layers.17.mlp.fc1.weight', 'text_model.encoder.layers.10.layer_norm1.bias', 'text_model.encoder.layers.5.self_attn.q_proj.bias', 'text_model.encoder.layers.2.mlp.fc2.bias', 'text_model.encoder.layers.3.layer_norm2.weight', 'text_model.encoder.layers.21.self_attn.v_proj.weight', 'text_model.encoder.layers.3.mlp.fc1.bias', 'text_model.encoder.layers.7.self_attn.k_proj.weight', 'text_model.encoder.layers.21.self_attn.q_proj.weight', 'text_model.encoder.layers.0.layer_norm2.bias', 'text_model.encoder.layers.14.mlp.fc2.bias', 'text_model.encoder.layers.21.layer_norm1.bias', 'text_model.encoder.layers.17.self_attn.v_proj.bias', 'text_model.encoder.layers.6.mlp.fc1.bias', 'text_model.encoder.layers.1.mlp.fc2.bias', 'text_model.encoder.layers.13.self_attn.v_proj.bias', 'text_model.encoder.layers.4.mlp.fc2.bias', 'text_model.encoder.layers.4.self_attn.q_proj.weight', 'text_model.encoder.layers.0.self_attn.q_proj.bias', 'text_model.encoder.layers.12.layer_norm1.weight', 'text_model.encoder.layers.3.mlp.fc2.bias', 'text_model.encoder.layers.10.mlp.fc1.weight', 'logit_scale', 'text_model.encoder.layers.21.layer_norm1.weight', 'text_model.encoder.layers.4.layer_norm1.bias', 'text_model.encoder.layers.16.layer_norm2.bias', 'text_model.encoder.layers.9.self_attn.q_proj.bias', 'text_model.encoder.layers.2.self_attn.k_proj.bias', 'text_model.encoder.layers.6.self_attn.q_proj.weight', 'text_model.encoder.layers.16.self_attn.q_proj.weight', 'text_model.encoder.layers.8.self_attn.v_proj.weight', 'text_model.encoder.layers.12.self_attn.out_proj.weight', 'text_model.encoder.layers.3.mlp.fc2.weight', 'text_model.encoder.layers.9.layer_norm1.bias', 'text_model.encoder.layers.22.self_attn.v_proj.bias', 'text_model.encoder.layers.11.layer_norm2.weight', 'text_model.encoder.layers.9.mlp.fc2.weight', 'text_model.encoder.layers.2.layer_norm2.weight', 'text_model.encoder.layers.12.self_attn.k_proj.bias', 'text_model.encoder.layers.0.self_attn.k_proj.weight', 'text_model.encoder.layers.15.mlp.fc1.weight', 'text_model.encoder.layers.10.self_attn.k_proj.weight', 'text_model.encoder.layers.11.self_attn.out_proj.bias', 'text_model.encoder.layers.12.layer_norm2.weight', 'text_model.encoder.layers.21.self_attn.k_proj.weight', 'text_model.encoder.layers.19.mlp.fc1.weight', 'text_model.encoder.layers.20.mlp.fc2.bias', 'text_model.encoder.layers.22.self_attn.q_proj.bias', 'text_model.encoder.layers.18.self_attn.k_proj.bias', 'text_model.encoder.layers.12.layer_norm1.bias', 'text_model.encoder.layers.5.layer_norm1.weight', 'text_model.encoder.layers.2.mlp.fc2.weight', 'text_model.encoder.layers.17.layer_norm1.bias', 'text_model.encoder.layers.5.self_attn.k_proj.bias', 'text_model.encoder.layers.16.layer_norm1.bias', 'text_model.encoder.layers.12.self_attn.k_proj.weight', 'text_model.encoder.layers.16.self_attn.q_proj.bias', 'text_model.encoder.layers.4.mlp.fc1.bias', 'text_model.encoder.layers.11.self_attn.k_proj.bias', 'text_model.encoder.layers.2.self_attn.v_proj.bias', 'text_model.encoder.layers.11.self_attn.v_proj.bias', 'text_model.encoder.layers.20.layer_norm2.bias', 'text_model.encoder.layers.16.self_attn.out_proj.weight', 'text_model.encoder.layers.0.self_attn.out_proj.bias', 'text_model.encoder.layers.20.self_attn.out_proj.bias', 'text_model.encoder.layers.5.self_attn.k_proj.weight', 'text_model.encoder.layers.6.self_attn.v_proj.weight', 'text_model.encoder.layers.0.layer_norm2.weight', 'text_model.encoder.layers.10.layer_norm1.weight', 'text_model.encoder.layers.10.mlp.fc1.bias', 'text_model.encoder.layers.14.mlp.fc1.weight', 'text_model.encoder.layers.0.layer_norm1.weight', 'text_model.encoder.layers.18.self_attn.q_proj.weight', 'text_model.encoder.layers.3.self_attn.v_proj.weight', 'text_model.encoder.layers.20.self_attn.q_proj.weight', 'text_model.encoder.layers.6.self_attn.k_proj.weight', 'text_model.encoder.layers.8.layer_norm1.weight', 'text_model.encoder.layers.14.layer_norm1.weight', 'text_model.encoder.layers.8.self_attn.v_proj.bias', 'text_model.encoder.layers.13.self_attn.q_proj.weight', 'text_model.encoder.layers.8.mlp.fc1.bias', 'text_model.encoder.layers.11.layer_norm1.bias', 'text_model.encoder.layers.11.self_attn.out_proj.weight', 'text_model.encoder.layers.21.mlp.fc2.bias', 'text_model.encoder.layers.6.self_attn.out_proj.bias', 'text_model.encoder.layers.1.self_attn.k_proj.weight', 'text_model.encoder.layers.1.mlp.fc1.bias', 'text_model.encoder.layers.11.mlp.fc2.weight', 'text_model.encoder.layers.16.mlp.fc2.bias', 'text_model.encoder.layers.0.self_attn.v_proj.weight', 'text_model.encoder.layers.7.layer_norm1.weight', 'text_model.encoder.layers.10.mlp.fc2.weight', 'text_model.encoder.layers.20.self_attn.out_proj.weight', 'text_model.encoder.layers.0.layer_norm1.bias', 'text_model.encoder.layers.16.layer_norm1.weight', 'text_model.encoder.layers.9.self_attn.k_proj.bias', 'text_model.encoder.layers.15.self_attn.out_proj.weight', 'text_model.encoder.layers.23.mlp.fc1.weight', 'text_model.encoder.layers.9.mlp.fc1.bias', 'text_model.encoder.layers.15.self_attn.q_proj.weight', 'text_model.final_layer_norm.weight', 'text_model.encoder.layers.6.self_attn.q_proj.bias', 'text_model.encoder.layers.16.mlp.fc1.weight', 'text_model.encoder.layers.1.self_attn.out_proj.bias', 'text_model.encoder.layers.22.self_attn.v_proj.weight', 'text_model.encoder.layers.11.layer_norm1.weight', 'text_model.encoder.layers.1.mlp.fc1.weight', 'text_model.encoder.layers.18.self_attn.k_proj.weight', 'text_model.encoder.layers.13.layer_norm2.bias', 'text_model.encoder.layers.4.self_attn.k_proj.weight', 'text_model.encoder.layers.12.self_attn.out_proj.bias', 'text_model.encoder.layers.4.self_attn.k_proj.bias', 'text_model.encoder.layers.0.self_attn.q_proj.weight', 'text_model.encoder.layers.13.self_attn.k_proj.bias', 'text_model.encoder.layers.5.mlp.fc1.bias', 'text_model.encoder.layers.8.self_attn.out_proj.bias', 'text_model.encoder.layers.17.layer_norm2.weight', 'text_model.encoder.layers.1.self_attn.v_proj.weight', 'text_model.encoder.layers.7.layer_norm2.weight', 'text_model.encoder.layers.18.mlp.fc1.bias', 'text_model.encoder.layers.1.self_attn.v_proj.bias', 'text_model.encoder.layers.20.self_attn.v_proj.weight', 'text_projection.weight', 'text_model.encoder.layers.7.mlp.fc1.weight', 'text_model.encoder.layers.5.self_attn.v_proj.bias', 'text_model.encoder.layers.12.mlp.fc2.weight', 'text_model.encoder.layers.5.mlp.fc2.weight', 'text_model.encoder.layers.19.self_attn.v_proj.bias', 'text_model.encoder.layers.14.self_attn.out_proj.weight', 'text_model.encoder.layers.14.mlp.fc1.bias', 'text_model.encoder.layers.3.self_attn.k_proj.bias', 'text_model.encoder.layers.6.layer_norm2.bias', 'text_model.encoder.layers.14.layer_norm1.bias', 'text_model.encoder.layers.22.mlp.fc1.bias', 'text_model.encoder.layers.17.layer_norm2.bias', 'text_model.encoder.layers.10.self_attn.v_proj.bias', 'text_model.encoder.layers.12.mlp.fc1.bias', 'text_model.encoder.layers.4.self_attn.out_proj.weight', 'text_model.encoder.layers.3.layer_norm1.weight', 'text_model.encoder.layers.10.self_attn.q_proj.bias', 'text_model.encoder.layers.10.self_attn.out_proj.weight', 'text_model.encoder.layers.17.self_attn.k_proj.bias', 'text_model.encoder.layers.21.mlp.fc2.weight', 'text_model.encoder.layers.13.layer_norm2.weight', 'text_model.encoder.layers.7.mlp.fc1.bias', 'text_model.encoder.layers.6.layer_norm1.weight', 'text_model.encoder.layers.7.self_attn.out_proj.bias', 'text_model.encoder.layers.9.mlp.fc1.weight', 'text_model.encoder.layers.17.mlp.fc2.bias', 'text_model.encoder.layers.11.self_attn.v_proj.weight', 'text_model.encoder.layers.21.mlp.fc1.weight', 'text_model.encoder.layers.15.self_attn.q_proj.bias', 'text_model.encoder.layers.1.self_attn.q_proj.weight', 'text_model.encoder.layers.17.self_attn.k_proj.weight', 'text_model.encoder.layers.0.self_attn.k_proj.bias', 'text_model.encoder.layers.8.self_attn.q_proj.weight', 'text_model.encoder.layers.9.self_attn.v_proj.bias', 'text_model.encoder.layers.6.mlp.fc2.weight', 'text_model.encoder.layers.0.self_attn.out_proj.weight', 'text_model.encoder.layers.18.self_attn.out_proj.bias', 'text_model.encoder.layers.7.mlp.fc2.bias', 'text_model.encoder.layers.2.layer_norm1.weight', 'text_model.encoder.layers.1.layer_norm2.weight', 'text_model.encoder.layers.6.self_attn.v_proj.bias', 'text_model.encoder.layers.9.self_attn.out_proj.bias', 'text_model.encoder.layers.22.layer_norm1.weight', 'text_model.encoder.layers.13.layer_norm1.weight', 'text_model.encoder.layers.23.layer_norm2.bias', 'text_model.encoder.layers.3.layer_norm2.bias', 'text_model.encoder.layers.9.layer_norm2.bias', 'text_model.encoder.layers.17.self_attn.q_proj.bias', 'text_model.encoder.layers.18.layer_norm1.weight', 'text_model.encoder.layers.21.self_attn.out_proj.bias', 'text_model.encoder.layers.1.self_attn.k_proj.bias', 'text_model.encoder.layers.14.self_attn.out_proj.bias', 'text_model.encoder.layers.21.layer_norm2.bias', 'text_model.encoder.layers.23.mlp.fc2.weight', 'text_model.encoder.layers.15.self_attn.v_proj.weight', 'text_model.encoder.layers.23.self_attn.q_proj.bias', 'text_model.encoder.layers.4.self_attn.out_proj.bias', 'text_model.encoder.layers.10.layer_norm2.weight', 'text_model.encoder.layers.13.layer_norm1.bias', 'text_model.encoder.layers.19.self_attn.k_proj.weight', 'text_model.encoder.layers.6.mlp.fc1.weight', 'text_model.encoder.layers.5.mlp.fc1.weight', 'text_model.encoder.layers.15.layer_norm2.weight', 'text_model.encoder.layers.12.mlp.fc2.bias', 'text_model.encoder.layers.5.self_attn.q_proj.weight', 'text_model.encoder.layers.16.layer_norm2.weight', 'text_model.encoder.layers.0.mlp.fc2.bias', 'text_model.encoder.layers.3.mlp.fc1.weight', 'text_model.encoder.layers.7.self_attn.q_proj.bias', 'text_model.encoder.layers.23.self_attn.q_proj.weight', 'text_model.encoder.layers.8.self_attn.k_proj.bias', 'text_model.encoder.layers.2.self_attn.v_proj.weight', 'text_model.encoder.layers.22.layer_norm2.bias', 'text_model.encoder.layers.22.self_attn.k_proj.bias', 'text_model.encoder.layers.17.self_attn.out_proj.weight', 'text_model.encoder.layers.8.self_attn.q_proj.bias', 'text_model.encoder.layers.5.layer_norm1.bias', 'text_model.encoder.layers.18.self_attn.v_proj.bias', 'text_model.encoder.layers.5.layer_norm2.bias', 'text_model.encoder.layers.13.self_attn.v_proj.weight', 'text_model.encoder.layers.14.self_attn.q_proj.weight', 'text_model.encoder.layers.10.layer_norm2.bias', 'text_model.encoder.layers.16.self_attn.k_proj.bias', 'text_model.encoder.layers.8.mlp.fc2.weight', 'text_model.encoder.layers.23.self_attn.k_proj.weight', 'text_model.encoder.layers.19.layer_norm1.bias', 'text_model.encoder.layers.2.self_attn.out_proj.bias', 'text_model.encoder.layers.7.self_attn.v_proj.weight', 'text_model.encoder.layers.16.self_attn.k_proj.weight', 'text_model.encoder.layers.7.layer_norm1.bias', 'text_model.encoder.layers.0.mlp.fc1.weight', 'text_model.encoder.layers.6.layer_norm1.bias', 'text_model.encoder.layers.5.self_attn.v_proj.weight', 'text_model.encoder.layers.12.self_attn.q_proj.weight', 'text_model.encoder.layers.20.layer_norm2.weight', 'text_model.encoder.layers.4.layer_norm2.bias', 'text_model.encoder.layers.8.layer_norm2.bias', 'text_model.encoder.layers.21.self_attn.out_proj.weight', 'text_model.encoder.layers.9.layer_norm1.weight', 'text_model.encoder.layers.11.mlp.fc1.bias', 'text_model.encoder.layers.20.self_attn.v_proj.bias', 'text_model.encoder.layers.19.mlp.fc2.weight', 'text_model.encoder.layers.5.self_attn.out_proj.bias', 'text_model.encoder.layers.1.self_attn.q_proj.bias', 'text_model.encoder.layers.12.self_attn.v_proj.bias', 'text_model.encoder.layers.14.mlp.fc2.weight', 'text_model.encoder.layers.13.mlp.fc2.bias', 'text_model.encoder.layers.12.mlp.fc1.weight', 'text_model.encoder.layers.13.mlp.fc2.weight', 'text_model.encoder.layers.21.self_attn.k_proj.bias', 'text_model.encoder.layers.21.layer_norm2.weight', 'text_model.encoder.layers.19.self_attn.k_proj.bias', 'text_model.encoder.layers.13.self_attn.out_proj.bias', 'text_model.encoder.layers.18.self_attn.q_proj.bias', 'text_model.encoder.layers.1.layer_norm2.bias', 'text_model.encoder.layers.14.layer_norm2.weight', 'text_model.encoder.layers.22.self_attn.q_proj.weight', 'text_model.encoder.layers.13.mlp.fc1.weight', 'text_model.encoder.layers.7.mlp.fc2.weight', 'text_model.encoder.layers.0.mlp.fc1.bias', 'text_model.encoder.layers.19.self_attn.q_proj.bias', 'text_model.encoder.layers.17.layer_norm1.weight', 'text_model.encoder.layers.20.mlp.fc1.weight', 'text_model.encoder.layers.11.mlp.fc1.weight', 'text_model.encoder.layers.4.self_attn.q_proj.bias', 'text_model.embeddings.position_ids', 'text_model.encoder.layers.15.mlp.fc1.bias', 'text_model.encoder.layers.3.self_attn.out_proj.bias', 'text_model.encoder.layers.19.layer_norm2.weight', 'text_model.encoder.layers.8.self_attn.k_proj.weight', 'text_model.encoder.layers.22.self_attn.out_proj.bias', 'text_model.encoder.layers.15.layer_norm1.bias', 'text_model.encoder.layers.7.self_attn.v_proj.bias', 'text_model.encoder.layers.22.mlp.fc2.bias', 'text_model.encoder.layers.6.layer_norm2.weight', 'text_model.encoder.layers.10.self_attn.out_proj.bias', 'text_model.encoder.layers.23.mlp.fc2.bias', 'text_model.encoder.layers.15.self_attn.k_proj.bias', 'text_model.encoder.layers.4.self_attn.v_proj.weight', 'text_model.encoder.layers.12.self_attn.q_proj.bias', 'text_model.encoder.layers.19.layer_norm2.bias', 'text_model.encoder.layers.20.self_attn.k_proj.weight', 'text_model.encoder.layers.5.self_attn.out_proj.weight', 'text_model.encoder.layers.17.self_attn.out_proj.bias', 'text_model.encoder.layers.23.mlp.fc1.bias', 'text_model.encoder.layers.21.mlp.fc1.bias', 'text_model.encoder.layers.17.self_attn.q_proj.weight', 'text_model.encoder.layers.11.layer_norm2.bias', 'text_model.encoder.layers.6.mlp.fc2.bias', 'text_model.encoder.layers.7.self_attn.out_proj.weight', 'text_model.embeddings.token_embedding.weight', 'text_model.encoder.layers.13.self_attn.q_proj.bias', 'text_model.encoder.layers.19.mlp.fc1.bias', 'text_model.encoder.layers.8.mlp.fc1.weight', 'text_model.encoder.layers.18.layer_norm1.bias', 'text_model.encoder.layers.2.layer_norm1.bias', 'text_model.encoder.layers.15.mlp.fc2.weight', 'text_model.encoder.layers.22.layer_norm2.weight', 'text_model.encoder.layers.10.self_attn.k_proj.bias', 'text_model.encoder.layers.20.self_attn.q_proj.bias', 'text_model.encoder.layers.23.layer_norm2.weight', 'text_model.encoder.layers.23.self_attn.out_proj.weight', 'text_model.encoder.layers.1.mlp.fc2.weight', 'text_model.encoder.layers.2.mlp.fc1.weight', 'text_model.encoder.layers.14.self_attn.v_proj.weight', 'text_model.encoder.layers.1.layer_norm1.weight', 'text_model.encoder.layers.3.self_attn.q_proj.weight', 'text_model.encoder.layers.13.self_attn.k_proj.weight', 'text_model.encoder.layers.1.self_attn.out_proj.weight', 'text_model.encoder.layers.22.mlp.fc2.weight', 'text_model.encoder.layers.5.mlp.fc2.bias', 'text_model.encoder.layers.11.self_attn.q_proj.bias', 'text_model.encoder.layers.17.mlp.fc2.weight', 'text_model.encoder.layers.17.self_attn.v_proj.weight', 'text_model.encoder.layers.9.layer_norm2.weight', 'text_model.encoder.layers.22.self_attn.k_proj.weight', 'text_model.encoder.layers.23.self_attn.k_proj.bias', 'text_model.encoder.layers.11.mlp.fc2.bias', 'text_model.encoder.layers.23.self_attn.out_proj.bias', 'text_model.encoder.layers.20.mlp.fc1.bias', 'text_model.encoder.layers.3.self_attn.k_proj.weight', 'text_model.encoder.layers.7.self_attn.k_proj.bias', 'text_model.encoder.layers.12.layer_norm2.bias', 'text_model.encoder.layers.2.self_attn.q_proj.bias', 'text_model.encoder.layers.16.mlp.fc1.bias', 'text_model.encoder.layers.18.self_attn.v_proj.weight', 'text_model.encoder.layers.20.mlp.fc2.weight', 'text_model.embeddings.position_embedding.weight', 'text_model.encoder.layers.15.self_attn.v_proj.bias', 'text_model.encoder.layers.13.self_attn.out_proj.weight', 'text_model.encoder.layers.15.self_attn.k_proj.weight', 'text_model.encoder.layers.14.self_attn.k_proj.weight', 'text_model.encoder.layers.21.self_attn.q_proj.bias', 'text_model.encoder.layers.20.self_attn.k_proj.bias', 'text_model.encoder.layers.13.mlp.fc1.bias', 'text_model.encoder.layers.9.self_attn.k_proj.weight', 'text_model.encoder.layers.6.self_attn.out_proj.weight', 'text_model.encoder.layers.12.self_attn.v_proj.weight', 'text_model.encoder.layers.15.self_attn.out_proj.bias']\r\n",
      "- This IS expected if you are initializing CLIPVisionModelWithProjection from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\r\n",
      "- This IS NOT expected if you are initializing CLIPVisionModelWithProjection from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\r\n",
      "preprocessor_config.json: 100%|████████████████| 316/316 [00:00<00:00, 1.54MB/s]\r\n",
      "tokenizer_config.json: 100%|███████████████████| 904/904 [00:00<00:00, 3.14MB/s]\r\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\r\n",
      "vocab.json: 100%|████████████████████████████| 862k/862k [00:00<00:00, 3.42MB/s]\r\n",
      "merges.txt: 100%|████████████████████████████| 525k/525k [00:00<00:00, 2.12MB/s]\r\n",
      "tokenizer.json: 100%|██████████████████████| 2.22M/2.22M [00:00<00:00, 13.1MB/s]\r\n",
      "special_tokens_map.json: 100%|█████████████████| 389/389 [00:00<00:00, 1.64MB/s]\r\n",
      "tokenizer/vocab.json: 100%|████████████████| 1.06M/1.06M [00:00<00:00, 40.7MB/s]\r\n",
      "tokenizer/merges.txt: 100%|███████████████████| 525k/525k [00:00<00:00, 140MB/s]\r\n",
      "tokenizer/special_tokens_map.json: 100%|███████| 460/460 [00:00<00:00, 2.33MB/s]\r\n",
      "tokenizer/tokenizer_config.json: 100%|█████████| 829/829 [00:00<00:00, 4.83MB/s]\r\n",
      "/opt/conda/lib/python3.10/site-packages/torch/hub.py:293: UserWarning: You are about to download and run code from an untrusted repository. In a future release, this won't be allowed. To add the repository to your trusted list, change the command to {calling_fn}(..., trust_repo=False) and a command prompt will appear asking for an explicit confirmation of trust, or load(..., trust_repo=True), which will assume that the prompt is to be answered with 'yes'. You can also use load(..., trust_repo='check') which will only prompt for confirmation if the repo is not already trusted. This will eventually be the default behaviour\r\n",
      "  warnings.warn(\r\n",
      "Downloading: \"https://github.com/miccunifi/ladi-vton/zipball/master\" to /root/.cache/torch/hub/master.zip\r\n",
      "unet/config.json: 100%|████████████████████████| 914/914 [00:00<00:00, 4.48MB/s]\r\n",
      "Downloading: \"https://github.com/miccunifi/ladi-vton/releases/download/weights/unet_vitonhd.pth\" to /root/.cache/torch/hub/checkpoints/unet_vitonhd.pth\r\n",
      "100%|██████████████████████████████████████| 1.61G/1.61G [01:04<00:00, 26.8MB/s]\r\n",
      "Using cache found in /root/.cache/torch/hub/miccunifi_ladi-vton_master\r\n",
      "Downloading: \"https://github.com/miccunifi/ladi-vton/releases/download/weights/emasc_vitonhd.pth\" to /root/.cache/torch/hub/checkpoints/emasc_vitonhd.pth\r\n",
      "100%|██████████████████████████████████████| 30.4M/30.4M [00:01<00:00, 21.1MB/s]\r\n",
      "Using cache found in /root/.cache/torch/hub/miccunifi_ladi-vton_master\r\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\r\n",
      "Downloading: \"https://github.com/miccunifi/ladi-vton/releases/download/weights/inversion_adapter_vitonhd.pth\" to /root/.cache/torch/hub/checkpoints/inversion_adapter_vitonhd.pth\r\n",
      "100%|████████████████████████████████████████| 520M/520M [00:13<00:00, 40.1MB/s]\r\n",
      "Using cache found in /root/.cache/torch/hub/miccunifi_ladi-vton_master\r\n",
      "initialization method [normal]\r\n",
      "initialization method [normal]\r\n",
      "Downloading: \"https://github.com/miccunifi/ladi-vton/releases/download/weights/warping_vitonhd.pth\" to /root/.cache/torch/hub/checkpoints/warping_vitonhd.pth\r\n",
      "100%|████████████████████████████████████████| 276M/276M [00:05<00:00, 48.9MB/s]\r\n",
      "  0%|                                                     | 0/1 [00:00<?, ?it/s]Precomputed warped clothing image is loaded\r\n",
      "\r\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\r\n",
      "  2%|▉                                           | 1/50 [00:00<00:17,  2.86it/s]\u001b[A\r\n",
      "  4%|█▊                                          | 2/50 [00:00<00:12,  4.00it/s]\u001b[A\r\n",
      "  6%|██▋                                         | 3/50 [00:00<00:10,  4.58it/s]\u001b[A\r\n",
      "  8%|███▌                                        | 4/50 [00:00<00:09,  4.91it/s]\u001b[A\r\n",
      " 10%|████▍                                       | 5/50 [00:01<00:08,  5.12it/s]\u001b[A\r\n",
      " 12%|█████▎                                      | 6/50 [00:01<00:08,  5.26it/s]\u001b[A\r\n",
      " 14%|██████▏                                     | 7/50 [00:01<00:08,  5.35it/s]\u001b[A\r\n",
      " 16%|███████                                     | 8/50 [00:01<00:07,  5.41it/s]\u001b[A\r\n",
      " 18%|███████▉                                    | 9/50 [00:01<00:07,  5.45it/s]\u001b[A\r\n",
      " 20%|████████▌                                  | 10/50 [00:01<00:07,  5.47it/s]\u001b[A\r\n",
      " 22%|█████████▍                                 | 11/50 [00:02<00:07,  5.49it/s]\u001b[A\r\n",
      " 24%|██████████▎                                | 12/50 [00:02<00:06,  5.49it/s]\u001b[A\r\n",
      " 26%|███████████▏                               | 13/50 [00:02<00:06,  5.50it/s]\u001b[A\r\n",
      " 28%|████████████                               | 14/50 [00:02<00:06,  5.51it/s]\u001b[A\r\n",
      " 30%|████████████▉                              | 15/50 [00:02<00:06,  5.52it/s]\u001b[A\r\n",
      " 32%|█████████████▊                             | 16/50 [00:03<00:06,  5.52it/s]\u001b[A\r\n",
      " 34%|██████████████▌                            | 17/50 [00:03<00:05,  5.52it/s]\u001b[A\r\n",
      " 36%|███████████████▍                           | 18/50 [00:03<00:05,  5.52it/s]\u001b[A\r\n",
      " 38%|████████████████▎                          | 19/50 [00:03<00:05,  5.53it/s]\u001b[A\r\n",
      " 40%|█████████████████▏                         | 20/50 [00:03<00:05,  5.53it/s]\u001b[A\r\n",
      " 42%|██████████████████                         | 21/50 [00:03<00:05,  5.53it/s]\u001b[A\r\n",
      " 44%|██████████████████▉                        | 22/50 [00:04<00:05,  5.53it/s]\u001b[A\r\n",
      " 46%|███████████████████▊                       | 23/50 [00:04<00:04,  5.53it/s]\u001b[A\r\n",
      " 48%|████████████████████▋                      | 24/50 [00:04<00:04,  5.53it/s]\u001b[A\r\n",
      " 50%|█████████████████████▌                     | 25/50 [00:04<00:04,  5.53it/s]\u001b[A\r\n",
      " 52%|██████████████████████▎                    | 26/50 [00:04<00:04,  5.53it/s]\u001b[A\r\n",
      " 54%|███████████████████████▏                   | 27/50 [00:05<00:04,  5.53it/s]\u001b[A\r\n",
      " 56%|████████████████████████                   | 28/50 [00:05<00:03,  5.53it/s]\u001b[A\r\n",
      " 58%|████████████████████████▉                  | 29/50 [00:05<00:03,  5.53it/s]\u001b[A\r\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:05<00:03,  5.53it/s]\u001b[A\r\n",
      " 62%|██████████████████████████▋                | 31/50 [00:05<00:03,  5.53it/s]\u001b[A\r\n",
      " 64%|███████████████████████████▌               | 32/50 [00:05<00:03,  5.53it/s]\u001b[A\r\n",
      " 66%|████████████████████████████▍              | 33/50 [00:06<00:03,  5.53it/s]\u001b[A\r\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:06<00:02,  5.53it/s]\u001b[A\r\n",
      " 70%|██████████████████████████████             | 35/50 [00:06<00:02,  5.53it/s]\u001b[A\r\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:06<00:02,  5.52it/s]\u001b[A\r\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:06<00:02,  5.53it/s]\u001b[A\r\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:07<00:02,  5.53it/s]\u001b[A\r\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:07<00:01,  5.53it/s]\u001b[A\r\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:07<00:01,  5.53it/s]\u001b[A\r\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:07<00:01,  5.53it/s]\u001b[A\r\n",
      " 84%|████████████████████████████████████       | 42/50 [00:07<00:01,  5.54it/s]\u001b[A\r\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:07<00:01,  5.54it/s]\u001b[A\r\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:08<00:01,  5.54it/s]\u001b[A\r\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:08<00:00,  5.54it/s]\u001b[A\r\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:08<00:00,  5.53it/s]\u001b[A\r\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:08<00:00,  5.53it/s]\u001b[A\r\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:08<00:00,  5.53it/s]\u001b[A\r\n",
      " 98%|██████████████████████████████████████████▏| 49/50 [00:09<00:00,  5.53it/s]\u001b[A\r\n",
      "100%|███████████████████████████████████████████| 50/50 [00:09<00:00,  5.43it/s]\r\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\r\n",
      "  return F.conv2d(input, weight, bias, self.stride,\r\n",
      "10698_00.jpg\r\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:11<00:00, 11.94s/it]\r\n"
     ]
    }
   ],
   "source": [
    "!python3 /kaggle/working/ladi-vton-hunting/src/inference.py --dataset vitonhd  --vitonhd_dataroot /kaggle/input/high-resolution-viton-zalando-dataset --output_dir /kaggle/working/ --test_order unpaired --category  upper_body --mixed_precision no --enable_xformers_memory_efficient_attention --use_png"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 3855472,
     "sourceId": 6683799,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4798047,
     "sourceId": 8120340,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30674,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 359.658367,
   "end_time": "2024-06-26T22:17:13.099548",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-06-26T22:11:13.441181",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
